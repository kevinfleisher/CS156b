{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming image_df is your DataFrame with filename and image_data columns\n",
    "# Function to display images from pixel data\n",
    "def display_image(pixel_data):\n",
    "    plt.imshow(pixel_data, cmap='gray')  # Display the image using grayscale colormap\n",
    "    plt.axis('off')  # Turn off axis labels\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in data subset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Data Subset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          filename                                         image_data\n",
      "0    image_178.jpg  [[33, 32, 32, 30, 31, 31, 29, 32, 32, 29, 32, ...\n",
      "1    image_150.jpg  [[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,...\n",
      "2    image_144.jpg  [[33, 29, 27, 27, 26, 24, 21, 18, 15, 4, 1, 0,...\n",
      "3     image_15.jpg  [[22, 19, 17, 17, 17, 18, 17, 17, 17, 20, 19, ...\n",
      "4     image_29.jpg  [[102, 106, 115, 96, 99, 119, 93, 102, 123, 10...\n",
      "..             ...                                                ...\n",
      "174   image_30.jpg  [[37, 37, 38, 38, 38, 37, 36, 35, 38, 36, 37, ...\n",
      "175   image_18.jpg  [[21, 22, 23, 22, 17, 16, 23, 16, 16, 3, 17, 1...\n",
      "176  image_149.jpg  [[13, 14, 15, 13, 12, 3, 3, 5, 5, 1, 5, 5, 11,...\n",
      "177  image_161.jpg  [[11, 12, 13, 13, 12, 12, 12, 12, 10, 13, 13, ...\n",
      "178  image_175.jpg  [[29, 55, 31, 21, 28, 66, 37, 65, 62, 66, 67, ...\n",
      "\n",
      "[179 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Directory containing JPEG images\n",
    "image_dir = '/Users/matthewdavidson/CS156bProject/data_subset/train_data_subset'\n",
    "\n",
    "# List to store image data\n",
    "image_data = []\n",
    "\n",
    "# Loop through all files in the directory\n",
    "for filename in os.listdir(image_dir):\n",
    "    if filename.endswith('.jpg'):\n",
    "        # Construct the full path to the image file\n",
    "        file_path = os.path.join(image_dir, filename)\n",
    "        \n",
    "        # Open the image using PIL\n",
    "        img = Image.open(file_path)\n",
    "        \n",
    "        # Convert the image to a NumPy array or any other data processing you need\n",
    "        img_array = np.array(img)\n",
    "        \n",
    "        # Append the image array and filename to the image_data list\n",
    "        image_data.append({'filename': filename, 'image_data': img_array})\n",
    "\n",
    "# Convert the image data list to a pandas DataFrame\n",
    "image_df = pd.DataFrame(image_data)\n",
    "\n",
    "# Print the DataFrame (optional)\n",
    "print(image_df)\n",
    "\n",
    "# Now you can perform data analysis on the image_df DataFrame\n",
    "# For example, you can access image data by filename:\n",
    "# image_row = image_df[image_df['filename'] == 'image_0.jpg']\n",
    "# image_array = image_row['image_data'].values[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Label Data Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0.1  Unnamed: 0                                     Path     Sex  \\\n",
      "0             0           0  train/pid50512/study1/view1_frontal.jpg  Female   \n",
      "1             1           1  train/pid21580/study2/view1_frontal.jpg  Female   \n",
      "2             2           2  train/pid21580/study1/view1_frontal.jpg  Female   \n",
      "3             3           3  train/pid21580/study1/view2_lateral.jpg  Female   \n",
      "4             4           4  train/pid33839/study1/view1_frontal.jpg    Male   \n",
      "\n",
      "   Age Frontal/Lateral AP/PA  No Finding  Enlarged Cardiomediastinum  \\\n",
      "0   68         Frontal    AP         1.0                         NaN   \n",
      "1   87         Frontal    AP         NaN                         NaN   \n",
      "2   83         Frontal    AP         NaN                         NaN   \n",
      "3   83         Lateral   NaN         NaN                         NaN   \n",
      "4   41         Frontal    AP         NaN                         NaN   \n",
      "\n",
      "   Cardiomegaly  Lung Opacity  Pneumonia  Pleural Effusion  Pleural Other  \\\n",
      "0           NaN           NaN        NaN               NaN            NaN   \n",
      "1           0.0           1.0        NaN               0.0            NaN   \n",
      "2           NaN           1.0        NaN               NaN            NaN   \n",
      "3           NaN           1.0        NaN               NaN            NaN   \n",
      "4           NaN           NaN        NaN               NaN            NaN   \n",
      "\n",
      "   Fracture  Support Devices  \n",
      "0       NaN              1.0  \n",
      "1       1.0              NaN  \n",
      "2       1.0              NaN  \n",
      "3       1.0              NaN  \n",
      "4       NaN              NaN  \n"
     ]
    }
   ],
   "source": [
    "# File path to the CSV file\n",
    "csv_file = '/Users/matthewdavidson/CS156bProject/CS156b/Data/train2023.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "full_df = pd.read_csv(csv_file)\n",
    "print(full_df.head())\n",
    "\n",
    "# Extract the labels from the DataFrame\n",
    "labels = full_df.columns[6:]  # Assuming the labels start from the 7th column (index 6) onwards\n",
    "\n",
    "# Print the extracted labels\n",
    "#print(\"Extracted labels:\")\n",
    "#(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Label for Image Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['train/pid50512/study1/view1_frontal.jpg',\n",
       " 'train/pid29112/study1/view2_lateral.jpg',\n",
       " 'train/pid01032/study2/view1_frontal.jpg',\n",
       " 'train/pid22981/study1/view1_frontal.jpg',\n",
       " 'train/pid50403/study2/view1_frontal.jpg',\n",
       " 'train/pid16802/study3/view1_frontal.jpg',\n",
       " 'train/pid40709/study4/view2_lateral.jpg',\n",
       " 'train/pid21394/study1/view1_frontal.jpg',\n",
       " 'train/pid30713/study5/view1_frontal.jpg',\n",
       " 'train/pid05474/study1/view2_lateral.jpg',\n",
       " 'train/pid32143/study3/view1_frontal.jpg',\n",
       " 'train/pid12801/study14/view2_lateral.jpg',\n",
       " 'train/pid45962/study1/view1_frontal.jpg',\n",
       " 'train/pid43778/study32/view1_frontal.jpg',\n",
       " 'train/pid09135/study1/view1_frontal.jpg',\n",
       " 'train/pid11731/study5/view1_frontal.jpg',\n",
       " 'train/pid42849/study1/view2_lateral.jpg',\n",
       " 'train/pid34488/study2/view1_frontal.jpg',\n",
       " 'train/pid06670/study11/view1_frontal.jpg',\n",
       " 'train/pid42109/study1/view1_frontal.jpg',\n",
       " 'train/pid23745/study5/view2_lateral.jpg',\n",
       " 'train/pid41770/study3/view1_frontal.jpg',\n",
       " 'train/pid01641/study12/view1_frontal.jpg',\n",
       " 'train/pid34454/study2/view1_frontal.jpg',\n",
       " 'train/pid24428/study4/view1_frontal.jpg',\n",
       " 'train/pid26244/study1/view1_frontal.jpg',\n",
       " 'train/pid49885/study6/view1_frontal.jpg',\n",
       " 'train/pid29800/study4/view1_frontal.jpg',\n",
       " 'train/pid28219/study5/view1_frontal.jpg',\n",
       " 'train/pid43386/study2/view1_frontal.jpg',\n",
       " 'train/pid13851/study17/view1_frontal.jpg',\n",
       " 'train/pid26651/study2/view1_frontal.jpg',\n",
       " 'train/pid24926/study4/view1_frontal.jpg',\n",
       " 'train/pid05893/study2/view1_frontal.jpg',\n",
       " 'train/pid33633/study5/view1_frontal.jpg',\n",
       " 'train/pid41549/study1/view2_frontal.jpg',\n",
       " 'train/pid48548/study3/view1_frontal.jpg',\n",
       " 'train/pid41695/study1/view1_frontal.jpg',\n",
       " 'train/pid25146/study32/view1_frontal.jpg',\n",
       " 'train/pid16380/study3/view1_frontal.jpg',\n",
       " 'train/pid05104/study3/view1_frontal.jpg',\n",
       " 'train/pid20203/study1/view2_lateral.jpg',\n",
       " 'train/pid20083/study7/view1_frontal.jpg',\n",
       " 'train/pid41842/study8/view1_frontal.jpg',\n",
       " 'train/pid32558/study2/view1_frontal.jpg',\n",
       " 'train/pid46207/study22/view1_frontal.jpg',\n",
       " 'train/pid40258/study1/view1_frontal.jpg',\n",
       " 'train/pid28788/study4/view1_frontal.jpg',\n",
       " 'train/pid03377/study6/view2_lateral.jpg',\n",
       " 'train/pid21827/study9/view1_frontal.jpg',\n",
       " 'train/pid22545/study1/view1_frontal.jpg',\n",
       " 'train/pid35228/study24/view1_frontal.jpg',\n",
       " 'train/pid37039/study9/view1_frontal.jpg',\n",
       " 'train/pid39089/study16/view1_frontal.jpg',\n",
       " 'train/pid10353/study3/view1_frontal.jpg',\n",
       " 'train/pid20369/study1/view1_frontal.jpg',\n",
       " 'train/pid18628/study1/view1_frontal.jpg',\n",
       " 'train/pid11998/study2/view1_frontal.jpg',\n",
       " 'train/pid20073/study2/view2_lateral.jpg',\n",
       " 'train/pid13594/study10/view1_frontal.jpg',\n",
       " 'train/pid40494/study1/view1_frontal.jpg',\n",
       " 'train/pid13213/study4/view1_frontal.jpg',\n",
       " 'train/pid21657/study10/view1_frontal.jpg',\n",
       " 'train/pid32456/study1/view2_lateral.jpg',\n",
       " 'train/pid12839/study1/view2_lateral.jpg',\n",
       " 'train/pid06365/study10/view1_frontal.jpg',\n",
       " 'train/pid39221/study1/view1_frontal.jpg',\n",
       " 'train/pid46564/study2/view1_frontal.jpg',\n",
       " 'train/pid48180/study7/view1_frontal.jpg',\n",
       " 'train/pid04622/study1/view1_frontal.jpg',\n",
       " 'train/pid44440/study1/view1_frontal.jpg',\n",
       " 'train/pid40188/study3/view2_lateral.jpg',\n",
       " 'train/pid18510/study1/view1_frontal.jpg',\n",
       " 'train/pid38173/study2/view1_frontal.jpg',\n",
       " 'train/pid42211/study1/view1_frontal.jpg',\n",
       " 'train/pid08256/study1/view2_lateral.jpg',\n",
       " 'train/pid33552/study15/view1_frontal.jpg',\n",
       " 'train/pid45428/study2/view1_frontal.jpg',\n",
       " 'train/pid26762/study12/view1_frontal.jpg',\n",
       " 'train/pid41460/study1/view1_frontal.jpg',\n",
       " 'train/pid50597/study43/view2_lateral.jpg',\n",
       " 'train/pid05239/study3/view1_frontal.jpg',\n",
       " 'train/pid10166/study2/view1_frontal.jpg',\n",
       " 'train/pid45374/study1/view1_frontal.jpg',\n",
       " 'train/pid39481/study1/view1_frontal.jpg',\n",
       " 'train/pid15112/study12/view1_frontal.jpg',\n",
       " 'train/pid10634/study1/view2_lateral.jpg',\n",
       " 'train/pid27508/study1/view1_frontal.jpg',\n",
       " 'train/pid17298/study5/view1_frontal.jpg',\n",
       " 'train/pid50011/study6/view1_frontal.jpg',\n",
       " 'train/pid28869/study2/view1_frontal.jpg',\n",
       " 'train/pid13573/study1/view2_lateral.jpg',\n",
       " 'train/pid23010/study13/view2_lateral.jpg',\n",
       " 'train/pid47112/study4/view1_frontal.jpg',\n",
       " 'train/pid35993/study1/view1_frontal.jpg',\n",
       " 'train/pid22285/study2/view2_lateral.jpg',\n",
       " 'train/pid02273/study5/view1_frontal.jpg',\n",
       " 'train/pid20509/study31/view1_frontal.jpg',\n",
       " 'train/pid01025/study16/view1_frontal.jpg',\n",
       " 'train/pid38213/study1/view1_frontal.jpg',\n",
       " 'train/pid19968/study1/view1_frontal.jpg',\n",
       " 'train/pid24002/study2/view2_lateral.jpg',\n",
       " 'train/pid07204/study1/view1_frontal.jpg',\n",
       " 'train/pid35441/study3/view1_frontal.jpg',\n",
       " 'train/pid50072/study1/view1_frontal.jpg',\n",
       " 'train/pid09711/study1/view1_frontal.jpg',\n",
       " 'train/pid22210/study2/view2_lateral.jpg',\n",
       " 'train/pid00322/study6/view1_frontal.jpg',\n",
       " 'train/pid44295/study1/view2_lateral.jpg',\n",
       " 'train/pid03164/study1/view2_lateral.jpg',\n",
       " 'train/pid06427/study1/view1_frontal.jpg',\n",
       " 'train/pid14503/study3/view2_lateral.jpg',\n",
       " 'train/pid43259/study3/view1_frontal.jpg',\n",
       " 'train/pid26523/study4/view1_frontal.jpg',\n",
       " 'train/pid40229/study1/view1_frontal.jpg',\n",
       " 'train/pid35245/study10/view1_frontal.jpg',\n",
       " 'train/pid11559/study2/view1_frontal.jpg',\n",
       " 'train/pid16688/study1/view1_frontal.jpg',\n",
       " 'train/pid50300/study11/view1_frontal.jpg',\n",
       " 'train/pid17222/study5/view1_frontal.jpg',\n",
       " 'train/pid08719/study8/view1_frontal.jpg',\n",
       " 'train/pid30418/study3/view1_frontal.jpg',\n",
       " 'train/pid26768/study2/view1_frontal.jpg',\n",
       " 'train/pid49841/study26/view1_frontal.jpg',\n",
       " 'train/pid07538/study28/view1_frontal.jpg',\n",
       " 'train/pid13676/study7/view1_frontal.jpg',\n",
       " 'train/pid14604/study3/view1_frontal.jpg',\n",
       " 'train/pid05878/study11/view1_frontal.jpg',\n",
       " 'train/pid06271/study5/view1_frontal.jpg',\n",
       " 'train/pid17799/study1/view1_frontal.jpg',\n",
       " 'train/pid07471/study2/view1_frontal.jpg',\n",
       " 'train/pid20331/study7/view1_frontal.jpg',\n",
       " 'train/pid25121/study6/view1_frontal.jpg',\n",
       " 'train/pid11755/study1/view1_frontal.jpg',\n",
       " 'train/pid23386/study4/view1_frontal.jpg',\n",
       " 'train/pid14676/study5/view1_frontal.jpg',\n",
       " 'train/pid29390/study3/view1_frontal.jpg',\n",
       " 'train/pid12163/study1/view1_frontal.jpg',\n",
       " 'train/pid30014/study2/view1_frontal.jpg',\n",
       " 'train/pid46144/study1/view1_frontal.jpg',\n",
       " 'train/pid42309/study1/view1_frontal.jpg',\n",
       " 'train/pid12663/study6/view1_frontal.jpg',\n",
       " 'train/pid19401/study4/view1_frontal.jpg',\n",
       " 'train/pid31941/study2/view1_frontal.jpg',\n",
       " 'train/pid37173/study3/view1_frontal.jpg',\n",
       " 'train/pid10605/study4/view1_frontal.jpg',\n",
       " 'train/pid47614/study1/view1_frontal.jpg',\n",
       " 'train/pid03137/study5/view1_frontal.jpg',\n",
       " 'train/pid36158/study1/view1_frontal.jpg',\n",
       " 'train/pid31840/study3/view1_frontal.jpg',\n",
       " 'train/pid37861/study2/view1_frontal.jpg',\n",
       " 'train/pid45814/study3/view1_frontal.jpg',\n",
       " 'train/pid04474/study3/view1_frontal.jpg',\n",
       " 'train/pid50459/study2/view1_frontal.jpg',\n",
       " 'train/pid19142/study1/view1_frontal.jpg',\n",
       " 'train/pid05407/study1/view2_frontal.jpg',\n",
       " 'train/pid46350/study2/view1_frontal.jpg',\n",
       " 'train/pid01240/study1/view1_frontal.jpg',\n",
       " 'train/pid32060/study3/view1_frontal.jpg',\n",
       " 'train/pid00814/study1/view1_frontal.jpg',\n",
       " 'train/pid35577/study2/view1_frontal.jpg',\n",
       " 'train/pid29306/study3/view1_frontal.jpg',\n",
       " 'train/pid14372/study1/view1_frontal.jpg',\n",
       " 'train/pid35151/study4/view2_lateral.jpg',\n",
       " 'train/pid27657/study1/view1_frontal.jpg',\n",
       " 'train/pid41190/study1/view1_frontal.jpg',\n",
       " 'train/pid34911/study1/view1_frontal.jpg',\n",
       " 'train/pid48362/study1/view1_frontal.jpg',\n",
       " 'train/pid20223/study1/view1_frontal.jpg',\n",
       " 'train/pid05030/study1/view1_frontal.jpg',\n",
       " 'train/pid38168/study2/view1_frontal.jpg',\n",
       " 'train/pid23496/study1/view1_frontal.jpg',\n",
       " 'train/pid33741/study1/view1_frontal.jpg',\n",
       " 'train/pid13844/study2/view1_frontal.jpg',\n",
       " 'train/pid13224/study3/view1_frontal.jpg',\n",
       " 'train/pid23285/study1/view1_lateral.jpg',\n",
       " 'train/pid20230/study1/view1_frontal.jpg',\n",
       " 'train/pid20342/study2/view1_frontal.jpg',\n",
       " 'train/pid26570/study1/view1_frontal.jpg']"
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the NumPy array containing data\n",
    "data_array = np.load('/Users/matthewdavidson/CS156bProject/train_data_subset.npy')\n",
    "# Function to edit file paths\n",
    "def edit_file_path(path):\n",
    "    # Split the path by '/'\n",
    "    parts = path.split('/')\n",
    "    # Extract the relevant parts (assuming the format is consistent)\n",
    "    relevant_parts = parts[4:]  # Adjust the index based on your path structure\n",
    "    # Join the relevant parts to create the new path format\n",
    "    new_path = '/'.join(relevant_parts)\n",
    "    return new_path\n",
    "\n",
    "# Apply the edit_file_path function to each path in data_array\n",
    "path_array = [edit_file_path(path) for path in data_array]\n",
    "(path_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join Proper Labels with images in Image Subset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame of filtered labels:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AP/PA</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>Enlarged Cardiomediastinum</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Lung Opacity</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Pleural Effusion</th>\n",
       "      <th>Pleural Other</th>\n",
       "      <th>Fracture</th>\n",
       "      <th>Support Devices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AP</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>PA</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3000</th>\n",
       "      <td>AP</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4000</th>\n",
       "      <td>AP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>174000</th>\n",
       "      <td>AP</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>175000</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176000</th>\n",
       "      <td>AP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>177000</th>\n",
       "      <td>AP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>178000</th>\n",
       "      <td>AP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>179 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       AP/PA  No Finding  Enlarged Cardiomediastinum  Cardiomegaly  \\\n",
       "0         AP         1.0                         NaN           NaN   \n",
       "1000     NaN         1.0                         NaN           NaN   \n",
       "2000      PA         1.0                         NaN           NaN   \n",
       "3000      AP         1.0                        -1.0           NaN   \n",
       "4000      AP         NaN                         NaN           NaN   \n",
       "...      ...         ...                         ...           ...   \n",
       "174000    AP         1.0                         NaN           NaN   \n",
       "175000   NaN         NaN                         NaN           NaN   \n",
       "176000    AP         NaN                         NaN           NaN   \n",
       "177000    AP         NaN                         NaN           NaN   \n",
       "178000    AP         NaN                         1.0           NaN   \n",
       "\n",
       "        Lung Opacity  Pneumonia  Pleural Effusion  Pleural Other  Fracture  \\\n",
       "0                NaN        NaN               NaN            NaN       NaN   \n",
       "1000             NaN        NaN               0.0            NaN       NaN   \n",
       "2000             NaN        NaN              -1.0            NaN       NaN   \n",
       "3000            -1.0        NaN              -1.0            NaN       NaN   \n",
       "4000             NaN        NaN               NaN            NaN       NaN   \n",
       "...              ...        ...               ...            ...       ...   \n",
       "174000           NaN        NaN               NaN            NaN       NaN   \n",
       "175000           NaN        NaN               NaN            NaN       NaN   \n",
       "176000           1.0        NaN               1.0            NaN       NaN   \n",
       "177000           1.0        NaN               1.0            NaN       NaN   \n",
       "178000           1.0        NaN               NaN            NaN       NaN   \n",
       "\n",
       "        Support Devices  \n",
       "0                   1.0  \n",
       "1000                1.0  \n",
       "2000                NaN  \n",
       "3000                NaN  \n",
       "4000                NaN  \n",
       "...                 ...  \n",
       "174000              1.0  \n",
       "175000              1.0  \n",
       "176000              NaN  \n",
       "177000              1.0  \n",
       "178000              1.0  \n",
       "\n",
       "[179 rows x 10 columns]"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter full_df based on paths in data_array\n",
    "filtered_df = full_df[full_df['Path'].isin(path_array)]\n",
    "\n",
    "# Print the filtered DataFrame\n",
    "#print(filtered_df)\n",
    "\n",
    "# Extract the labels from the DataFrame\n",
    "filtered_labels = filtered_df.columns[6:]  # Assuming the labels start from the 7th column (index 6) onwards\n",
    "\n",
    "# Print the extracted labels\n",
    "#print(\"Extracted labels:\")\n",
    "#print(filtered_labels)\n",
    "\n",
    "# Extract only the columns containing label data\n",
    "label_columns = filtered_df.columns[6:]  # Assuming the labels start from the 7th column (index 6) onwards\n",
    "filtered_label_df = filtered_df[label_columns]\n",
    "\n",
    "# Print the filtered DataFrame containing only the label data\n",
    "print(\"DataFrame of filtered labels:\")\n",
    "(filtered_label_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merged Data frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged DataFrame:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>image_data</th>\n",
       "      <th>AP/PA</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>Enlarged Cardiomediastinum</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Lung Opacity</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Pleural Effusion</th>\n",
       "      <th>Pleural Other</th>\n",
       "      <th>Fracture</th>\n",
       "      <th>Support Devices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>image_178.jpg</td>\n",
       "      <td>[[33, 32, 32, 30, 31, 31, 29, 32, 32, 29, 32, ...</td>\n",
       "      <td>AP</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>image_150.jpg</td>\n",
       "      <td>[[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>image_144.jpg</td>\n",
       "      <td>[[33, 29, 27, 27, 26, 24, 21, 18, 15, 4, 1, 0,...</td>\n",
       "      <td>PA</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>image_15.jpg</td>\n",
       "      <td>[[22, 19, 17, 17, 17, 18, 17, 17, 17, 20, 19, ...</td>\n",
       "      <td>AP</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>image_29.jpg</td>\n",
       "      <td>[[102, 106, 115, 96, 99, 119, 93, 102, 123, 10...</td>\n",
       "      <td>AP</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        filename                                         image_data AP/PA  \\\n",
       "0  image_178.jpg  [[33, 32, 32, 30, 31, 31, 29, 32, 32, 29, 32, ...    AP   \n",
       "1  image_150.jpg  [[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,...   NaN   \n",
       "2  image_144.jpg  [[33, 29, 27, 27, 26, 24, 21, 18, 15, 4, 1, 0,...    PA   \n",
       "3   image_15.jpg  [[22, 19, 17, 17, 17, 18, 17, 17, 17, 20, 19, ...    AP   \n",
       "4   image_29.jpg  [[102, 106, 115, 96, 99, 119, 93, 102, 123, 10...    AP   \n",
       "\n",
       "   No Finding  Enlarged Cardiomediastinum  Cardiomegaly  Lung Opacity  \\\n",
       "0         1.0                         NaN           NaN           NaN   \n",
       "1         1.0                         NaN           NaN           NaN   \n",
       "2         1.0                         NaN           NaN           NaN   \n",
       "3         1.0                        -1.0           NaN          -1.0   \n",
       "4         NaN                         NaN           NaN           NaN   \n",
       "\n",
       "   Pneumonia  Pleural Effusion  Pleural Other  Fracture  Support Devices  \n",
       "0        NaN               NaN            NaN       NaN              1.0  \n",
       "1        NaN               0.0            NaN       NaN              1.0  \n",
       "2        NaN              -1.0            NaN       NaN              NaN  \n",
       "3        NaN              -1.0            NaN       NaN              NaN  \n",
       "4        NaN               NaN            NaN       NaN              NaN  "
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reset the index of both DataFrames to ensure they align row by row during concatenation\n",
    "filtered_labels_df = filtered_label_df.reset_index(drop=True, inplace=False)\n",
    "#print(filtered_labels_df)\n",
    "# Concatenate the DataFrames along axis 1 (columns), which will align them row by row\n",
    "merged_df = pd.concat([image_df, filtered_labels_df], axis=1)\n",
    "\n",
    "# Print the merged DataFrame to verify the merge\n",
    "print(\"Merged DataFrame:\")\n",
    "(merged_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert NaN values to 0\n",
    "\n",
    "used to denote uncertain or unspecified diagnosis to model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>image_data</th>\n",
       "      <th>AP/PA</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>Enlarged Cardiomediastinum</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Lung Opacity</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Pleural Effusion</th>\n",
       "      <th>Pleural Other</th>\n",
       "      <th>Fracture</th>\n",
       "      <th>Support Devices</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>image_178.jpg</td>\n",
       "      <td>[[33, 32, 32, 30, 31, 31, 29, 32, 32, 29, 32, ...</td>\n",
       "      <td>AP</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>image_150.jpg</td>\n",
       "      <td>[[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>image_144.jpg</td>\n",
       "      <td>[[33, 29, 27, 27, 26, 24, 21, 18, 15, 4, 1, 0,...</td>\n",
       "      <td>PA</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>image_15.jpg</td>\n",
       "      <td>[[22, 19, 17, 17, 17, 18, 17, 17, 17, 20, 19, ...</td>\n",
       "      <td>AP</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>image_29.jpg</td>\n",
       "      <td>[[102, 106, 115, 96, 99, 119, 93, 102, 123, 10...</td>\n",
       "      <td>AP</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        filename                                         image_data AP/PA  \\\n",
       "0  image_178.jpg  [[33, 32, 32, 30, 31, 31, 29, 32, 32, 29, 32, ...    AP   \n",
       "1  image_150.jpg  [[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,...     0   \n",
       "2  image_144.jpg  [[33, 29, 27, 27, 26, 24, 21, 18, 15, 4, 1, 0,...    PA   \n",
       "3   image_15.jpg  [[22, 19, 17, 17, 17, 18, 17, 17, 17, 20, 19, ...    AP   \n",
       "4   image_29.jpg  [[102, 106, 115, 96, 99, 119, 93, 102, 123, 10...    AP   \n",
       "\n",
       "   No Finding  Enlarged Cardiomediastinum  Cardiomegaly  Lung Opacity  \\\n",
       "0         1.0                         0.0           0.0           0.0   \n",
       "1         1.0                         0.0           0.0           0.0   \n",
       "2         1.0                         0.0           0.0           0.0   \n",
       "3         1.0                        -1.0           0.0          -1.0   \n",
       "4         0.0                         0.0           0.0           0.0   \n",
       "\n",
       "   Pneumonia  Pleural Effusion  Pleural Other  Fracture  Support Devices  \n",
       "0        0.0               0.0            0.0       0.0              1.0  \n",
       "1        0.0               0.0            0.0       0.0              1.0  \n",
       "2        0.0              -1.0            0.0       0.0              0.0  \n",
       "3        0.0              -1.0            0.0       0.0              0.0  \n",
       "4        0.0               0.0            0.0       0.0              0.0  "
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Assuming merged_df is your merged DataFrame\n",
    "\n",
    "# Replace NaN values with 0\n",
    "merged_df.fillna(0, inplace=True)\n",
    "#merged_df.drop(['level_0', 'index'], axis=1, inplace=True)\n",
    "\n",
    "# Print the modified DataFrame to verify the changes\n",
    "(merged_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram of pixel values for the first image in the DataFrame\n",
    "first_image_data = image_df.iloc[0]['image_data']  # Assuming the first image in the DataFrame\n",
    "pixel_values = first_image_data.flatten()\n",
    "\n",
    "# Plot the histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(pixel_values, bins=256, range=(0, 255), density=True, color='blue', alpha=0.7)\n",
    "plt.xlabel('Pixel Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Pixel Values')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Normalize the image data (assuming min-max normalization)\n",
    "normalized_image_data = (first_image_data - np.min(first_image_data)) / (np.max(first_image_data) - np.min(first_image_data))\n",
    "normalized_pixel_values = normalized_image_data.flatten()\n",
    "\n",
    "# Plot the histogram\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(normalized_pixel_values, bins=256, range=(0,1), density=True, color='blue', alpha=0.7)\n",
    "plt.xlabel('Pixel Value')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Normalized Pixel Values')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print the normalized image data (optional)\n",
    "print(\"Normalized Image Data:\")\n",
    "print(normalized_image_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image(first_image_data)\n",
    "print(f'first image data: {first_image_data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_image(normalized_image_data)\n",
    "print(f'normalized image data: {normalized_image_data}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfor index, row in image_df.iterrows():\\n    filename = row[\\'filename\\']\\n    image_data = row[\\'image_data\\']\\n    print(f\"Displaying image: {filename}\")\\n    display_image(image_data)\\n    '"
      ]
     },
     "execution_count": 258,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display images from image_df DataFrame\n",
    "'''\n",
    "for index, row in image_df.iterrows():\n",
    "    filename = row['filename']\n",
    "    image_data = row['image_data']\n",
    "    print(f\"Displaying image: {filename}\")\n",
    "    display_image(image_data)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dropout, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.image import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      ((tf.Tensor(33, shape=(), dtype=uint8), tf.Ten...\n",
      "1      ((tf.Tensor(2, shape=(), dtype=uint8), tf.Tens...\n",
      "2      ((tf.Tensor(33, shape=(), dtype=uint8), tf.Ten...\n",
      "3      ((tf.Tensor(22, shape=(), dtype=uint8), tf.Ten...\n",
      "4      ((tf.Tensor(102, shape=(), dtype=uint8), tf.Te...\n",
      "                             ...                        \n",
      "174    ((tf.Tensor(37, shape=(), dtype=uint8), tf.Ten...\n",
      "175    ((tf.Tensor(21, shape=(), dtype=uint8), tf.Ten...\n",
      "176    ((tf.Tensor(13, shape=(), dtype=uint8), tf.Ten...\n",
      "177    ((tf.Tensor(11, shape=(), dtype=uint8), tf.Ten...\n",
      "178    ((tf.Tensor(29, shape=(), dtype=uint8), tf.Ten...\n",
      "Name: image_data, Length: 179, dtype: object\n"
     ]
    }
   ],
   "source": [
    "merged_df['image_data'] = merged_df['image_data'].apply(tf.convert_to_tensor)\n",
    "print(merged_df['image_data'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2320\n",
      "2828\n",
      "179\n",
      "tf.Tensor(\n",
      "[[ 0  0  0 ... 13  8 11]\n",
      " [ 0  0  0 ... 10  8 11]\n",
      " [ 0  0  0 ... 10  9 12]\n",
      " ...\n",
      " [ 8 13 10 ...  2  0  0]\n",
      " [11 14 10 ...  0  0  0]\n",
      " [ 7 14 15 ...  1  2  2]], shape=(2048, 2494), dtype=uint8)\n"
     ]
    }
   ],
   "source": [
    "print(merged_df['image_data'][0].shape[0])\n",
    "print(merged_df['image_data'][0].shape[1])\n",
    "print(len(merged_df['image_data']))\n",
    "print(merged_df['image_data'][7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum height: 2828\n",
      "Maximum width: 3408\n"
     ]
    }
   ],
   "source": [
    "def get_shape(tensor):\n",
    "    return tensor.shape[0], tensor.shape[1]\n",
    "\n",
    "#max_height, max_width = merged_df['image_data'].apply(lambda x: get_shape(x),).max()\n",
    "max_height_array, max_width_array = [], []\n",
    "for i in merged_df['image_data']:\n",
    "    newmax_height, newmax_width = get_shape(i)\n",
    "    max_height_array.append(newmax_height)\n",
    "    max_width_array.append(newmax_width)\n",
    "\n",
    "\n",
    "print(\"Maximum height:\", max(max_height_array))\n",
    "print(\"Maximum width:\", max(max_width_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      ((tf.Tensor(33, shape=(), dtype=uint8), tf.Ten...\n",
      "1      ((tf.Tensor(2, shape=(), dtype=uint8), tf.Tens...\n",
      "2      ((tf.Tensor(33, shape=(), dtype=uint8), tf.Ten...\n",
      "3      ((tf.Tensor(22, shape=(), dtype=uint8), tf.Ten...\n",
      "4      ((tf.Tensor(102, shape=(), dtype=uint8), tf.Te...\n",
      "                             ...                        \n",
      "174    ((tf.Tensor(37, shape=(), dtype=uint8), tf.Ten...\n",
      "175    ((tf.Tensor(21, shape=(), dtype=uint8), tf.Ten...\n",
      "176    ((tf.Tensor(13, shape=(), dtype=uint8), tf.Ten...\n",
      "177    ((tf.Tensor(11, shape=(), dtype=uint8), tf.Ten...\n",
      "178    ((tf.Tensor(29, shape=(), dtype=uint8), tf.Ten...\n",
      "Name: padded_image, Length: 179, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Assuming merged_df contains your DataFrame with image tensors\n",
    "\n",
    "# Define the target height and width\n",
    "target_height = 2828\n",
    "target_width = 3408\n",
    "\n",
    "# Define a function to pad images\n",
    "def pad_image(image_tensor):\n",
    "    # Calculate the height and width difference\n",
    "    pad_height = target_height - image_tensor.shape[0]\n",
    "    pad_width = target_width - image_tensor.shape[1]\n",
    "\n",
    "    # Pad the image tensor with zeros\n",
    "    padded_image = tf.pad(image_tensor, paddings=[[0, pad_height], [0, pad_width]], mode='CONSTANT', constant_values=0)\n",
    "\n",
    "    return padded_image\n",
    "\n",
    "# Apply the pad_image function to all images in the DataFrame\n",
    "merged_df['padded_image'] = merged_df['image_data'].apply(pad_image)\n",
    "\n",
    "# Check the padded images\n",
    "print(merged_df['padded_image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[33 32 32 ...  0  0  0]\n",
      "  [32 31 31 ...  0  0  0]\n",
      "  [31 30 31 ...  0  0  0]\n",
      "  ...\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]]\n",
      "\n",
      " [[ 2  2  2 ...  0  0  0]\n",
      "  [ 2  2  2 ...  0  0  0]\n",
      "  [ 2  2  2 ...  0  0  0]\n",
      "  ...\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]]\n",
      "\n",
      " [[33 29 27 ...  0  0  0]\n",
      "  [32 28 26 ...  0  0  0]\n",
      "  [32 28 26 ...  0  0  0]\n",
      "  ...\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]]\n",
      "\n",
      " ...\n",
      "\n",
      " [[13 14 15 ...  0  0  0]\n",
      "  [16 11 14 ...  0  0  0]\n",
      "  [15  8 11 ...  0  0  0]\n",
      "  ...\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]]\n",
      "\n",
      " [[11 12 13 ...  0  0  0]\n",
      "  [13 12 11 ...  0  0  0]\n",
      "  [19 17 15 ...  0  0  0]\n",
      "  ...\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]]\n",
      "\n",
      " [[29 55 31 ...  0  0  0]\n",
      "  [66 18 23 ...  0  0  0]\n",
      "  [57 68 36 ...  0  0  0]\n",
      "  ...\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]\n",
      "  [ 0  0  0 ...  0  0  0]]]      No Finding  Enlarged Cardiomediastinum  Cardiomegaly  Lung Opacity  \\\n",
      "0           1.0                         0.0           0.0           0.0   \n",
      "1           1.0                         0.0           0.0           0.0   \n",
      "2           1.0                         0.0           0.0           0.0   \n",
      "3           1.0                        -1.0           0.0          -1.0   \n",
      "4           0.0                         0.0           0.0           0.0   \n",
      "..          ...                         ...           ...           ...   \n",
      "174         1.0                         0.0           0.0           0.0   \n",
      "175         0.0                         0.0           0.0           0.0   \n",
      "176         0.0                         0.0           0.0           1.0   \n",
      "177         0.0                         0.0           0.0           1.0   \n",
      "178         0.0                         1.0           0.0           1.0   \n",
      "\n",
      "     Pneumonia  Pleural Effusion  Pleural Other  Fracture  Support Devices  \\\n",
      "0          0.0               0.0            0.0       0.0              1.0   \n",
      "1          0.0               0.0            0.0       0.0              1.0   \n",
      "2          0.0              -1.0            0.0       0.0              0.0   \n",
      "3          0.0              -1.0            0.0       0.0              0.0   \n",
      "4          0.0               0.0            0.0       0.0              0.0   \n",
      "..         ...               ...            ...       ...              ...   \n",
      "174        0.0               0.0            0.0       0.0              1.0   \n",
      "175        0.0               0.0            0.0       0.0              1.0   \n",
      "176        0.0               1.0            0.0       0.0              0.0   \n",
      "177        0.0               1.0            0.0       0.0              1.0   \n",
      "178        0.0               0.0            0.0       0.0              1.0   \n",
      "\n",
      "     AP/PA_0  AP/PA_AP  AP/PA_PA  AP/PA_nan  \n",
      "0          0         1         0          0  \n",
      "1          1         0         0          0  \n",
      "2          0         0         1          0  \n",
      "3          0         1         0          0  \n",
      "4          0         1         0          0  \n",
      "..       ...       ...       ...        ...  \n",
      "174        0         1         0          0  \n",
      "175        1         0         0          0  \n",
      "176        0         1         0          0  \n",
      "177        0         1         0          0  \n",
      "178        0         1         0          0  \n",
      "\n",
      "[179 rows x 13 columns]\n"
     ]
    }
   ],
   "source": [
    "# Splitting the DataFrame into features (X) and labels (y)\n",
    "X = np.stack(merged_df['padded_image'])\n",
    "y = merged_df.drop(['filename', 'image_data','padded_image'], axis=1)\n",
    "\n",
    "# One-hot encode the labels\n",
    "y = pd.get_dummies(y, dummy_na=True)\n",
    "\n",
    "# Splitting the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_train = np.reshape(X_train, (*X_train.shape, 1))\n",
    "X_val = np.reshape(X_val, (*X_val.shape, 1))\n",
    "print(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Need to Specifiy the channel size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN architecture\n",
    "model = Sequential([\n",
    "    Conv2D(64, (3, 3), activation='relu', input_shape=X_train.shape[1:]),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dropout(0.2),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unspecified Data Resizing Attemps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "'images' must have either 3 or 4 dimensions.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[194], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resized_image\u001b[38;5;241m.\u001b[39mnumpy()  \u001b[38;5;66;03m# Convert back to NumPy array\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Resize all images and stack them into a NumPy array\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m resized_images \u001b[38;5;241m=\u001b[39m \u001b[43mmerged_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage_data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresize_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(resized_images\u001b[38;5;241m.\u001b[39mvalues)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4661\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4662\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4666\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4667\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4668\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4669\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4670\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4769\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4770\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/apply.py:1105\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m   1104\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1105\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/apply.py:1156\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1154\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1155\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m-> 1156\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1157\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1158\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1159\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1160\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1162\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1163\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1164\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1165\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/_libs/lib.pyx:2918\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[194], line 7\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resized_image\u001b[38;5;241m.\u001b[39mnumpy()  \u001b[38;5;66;03m# Convert back to NumPy array\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Resize all images and stack them into a NumPy array\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m resized_images \u001b[38;5;241m=\u001b[39m merged_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage_data\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mresize_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      8\u001b[0m X \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mstack(resized_images\u001b[38;5;241m.\u001b[39mvalues)\n",
      "Cell \u001b[0;32mIn[194], line 3\u001b[0m, in \u001b[0;36mresize_image\u001b[0;34m(image_tensor, target_size)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresize_image\u001b[39m(image_tensor, target_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m256\u001b[39m, \u001b[38;5;241m256\u001b[39m)):\n\u001b[0;32m----> 3\u001b[0m     resized_image \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m resized_image\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/image_ops_impl.py:1459\u001b[0m, in \u001b[0;36m_resize_images_common\u001b[0;34m(images, resizer_fn, size, preserve_aspect_ratio, name, skip_resize_if_same)\u001b[0m\n\u001b[1;32m   1457\u001b[0m   images \u001b[38;5;241m=\u001b[39m array_ops\u001b[38;5;241m.\u001b[39mexpand_dims(images, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m   1458\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m images\u001b[38;5;241m.\u001b[39mget_shape()\u001b[38;5;241m.\u001b[39mndims \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m4\u001b[39m:\n\u001b[0;32m-> 1459\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m must have either 3 or 4 dimensions.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1461\u001b[0m _, height, width, _ \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mget_shape()\u001b[38;5;241m.\u001b[39mas_list()\n\u001b[1;32m   1463\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mValueError\u001b[0m: 'images' must have either 3 or 4 dimensions."
     ]
    }
   ],
   "source": [
    "# Define a function to resize images\n",
    "def resize_image(image_tensor, target_size=(256, 256)):\n",
    "    resized_image = tf.image.resize(image_tensor, target_size)\n",
    "    return resized_image.numpy()  # Convert back to NumPy array\n",
    "\n",
    "# Resize all images and stack them into a NumPy array\n",
    "resized_images = merged_df['image_data'].apply(lambda x: resize_image(x))\n",
    "X = np.stack(resized_images.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2320, 2320, 2320, 2320, 2022, 2320, 2021, 2048, 2320, 2320, 2320, 2010, 2320, 2048, 2048, 2320, 2320, 2320, 2016, 2320, 2320, 2320, 2320, 2320, 2320, 2320, 2021, 2320, 2048, 2320, 2320, 2320, 2320, 2320, 2320, 2021, 2320, 2021, 1671, 2320, 2320, 2320, 2320, 2320, 2320, 2022, 2320, 2320, 2022, 2016, 1741, 2014, 2800, 2320, 2320, 2320, 2048, 2021, 2021, 2828, 2022, 1826, 2022, 2320, 2828, 2320, 2320, 2320, 2320, 2320, 2048, 2320, 2320, 2548, 2320, 2320, 2800, 2320, 2016, 2320, 2021, 2320, 2022, 2022, 2320, 2021, 2320, 2320, 2320, 2320, 2320, 2320, 2800, 2011, 1751, 2021, 2320, 2320, 2320, 2320, 2320, 2320, 2320, 2800, 2800, 1910, 2048, 2320, 2320, 2320, 1751, 2320, 1751, 2021, 2320, 2800, 2828, 2828, 2048, 2022, 2548, 2800, 1751, 2320, 2048, 2320, 2021, 2320, 2021, 2320, 2320, 2320, 1964, 2320, 2022, 1751, 2320, 1830, 1741, 1601, 2320, 2320, 2800, 2320, 2320, 2022, 2320, 2828, 2320, 2320, 2320, 2320, 2320, 2320, 2320, 2320, 2320, 2320, 2320, 2021, 2320, 1751, 2800, 2022, 2320, 2800, 2320, 2320, 2320, 2320, 2320, 2320, 1746, 1871, 2320, 2320, 2320, 2320, 2800]\n"
     ]
    }
   ],
   "source": [
    "# Assuming merge_df contains your merged DataFrame with image data and classification labels\n",
    "# Convert image_data from strings to numpy arrays\n",
    "#merged_df['image_data'] = merged_df['image_data'].apply(lambda x: np.array(eval(x)))\n",
    "\n",
    "dat_F_X = merged_df['image_data'].values\n",
    "\n",
    "image_lengths = [len(image_data) for image_data in merged_df['image_data'].values]\n",
    "print(image_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>image_data</th>\n",
       "      <th>AP/PA</th>\n",
       "      <th>No Finding</th>\n",
       "      <th>Enlarged Cardiomediastinum</th>\n",
       "      <th>Cardiomegaly</th>\n",
       "      <th>Lung Opacity</th>\n",
       "      <th>Pneumonia</th>\n",
       "      <th>Pleural Effusion</th>\n",
       "      <th>Pleural Other</th>\n",
       "      <th>Fracture</th>\n",
       "      <th>Support Devices</th>\n",
       "      <th>resized_image_data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>image_178.jpg</td>\n",
       "      <td>[[33, 32, 32, 30, 31, 31, 29, 32, 32, 29, 32, ...</td>\n",
       "      <td>AP</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[[33, 32, 32, 31, 30, 31, 31, 29, 31, 32, 31, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>image_150.jpg</td>\n",
       "      <td>[[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,...</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>image_144.jpg</td>\n",
       "      <td>[[33, 29, 27, 27, 26, 24, 21, 18, 15, 4, 1, 0,...</td>\n",
       "      <td>PA</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[33, 30, 28, 27, 27, 26, 24, 22, 19, 17, 13, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>image_15.jpg</td>\n",
       "      <td>[[22, 19, 17, 17, 17, 18, 17, 17, 17, 20, 19, ...</td>\n",
       "      <td>AP</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[22, 20, 18, 17, 17, 17, 18, 17, 17, 17, 18, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>image_29.jpg</td>\n",
       "      <td>[[102, 106, 115, 96, 99, 119, 93, 102, 123, 10...</td>\n",
       "      <td>AP</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[[101, 103, 109, 115, 101, 95, 101, 119, 104, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        filename                                         image_data AP/PA  \\\n",
       "0  image_178.jpg  [[33, 32, 32, 30, 31, 31, 29, 32, 32, 29, 32, ...    AP   \n",
       "1  image_150.jpg  [[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,...     0   \n",
       "2  image_144.jpg  [[33, 29, 27, 27, 26, 24, 21, 18, 15, 4, 1, 0,...    PA   \n",
       "3   image_15.jpg  [[22, 19, 17, 17, 17, 18, 17, 17, 17, 20, 19, ...    AP   \n",
       "4   image_29.jpg  [[102, 106, 115, 96, 99, 119, 93, 102, 123, 10...    AP   \n",
       "\n",
       "   No Finding  Enlarged Cardiomediastinum  Cardiomegaly  Lung Opacity  \\\n",
       "0         1.0                         0.0           0.0           0.0   \n",
       "1         1.0                         0.0           0.0           0.0   \n",
       "2         1.0                         0.0           0.0           0.0   \n",
       "3         1.0                        -1.0           0.0          -1.0   \n",
       "4         0.0                         0.0           0.0           0.0   \n",
       "\n",
       "   Pneumonia  Pleural Effusion  Pleural Other  Fracture  Support Devices  \\\n",
       "0        0.0               0.0            0.0       0.0              1.0   \n",
       "1        0.0               0.0            0.0       0.0              1.0   \n",
       "2        0.0              -1.0            0.0       0.0              0.0   \n",
       "3        0.0              -1.0            0.0       0.0              0.0   \n",
       "4        0.0               0.0            0.0       0.0              0.0   \n",
       "\n",
       "                                  resized_image_data  \n",
       "0  [[33, 32, 32, 31, 30, 31, 31, 29, 31, 32, 31, ...  \n",
       "1  [[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,...  \n",
       "2  [[33, 30, 28, 27, 27, 26, 24, 22, 19, 17, 13, ...  \n",
       "3  [[22, 20, 18, 17, 17, 17, 18, 17, 17, 17, 18, ...  \n",
       "4  [[101, 103, 109, 115, 101, 95, 101, 119, 104, ...  "
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "# Assuming merged_df contains your DataFrame with image data\n",
    "# Convert image_data from string to numpy array\n",
    "merged_df['image_data'] = merged_df['image_data'].apply(lambda x: np.array(x))\n",
    "\n",
    "# Find the maximum width and height among all images\n",
    "max_width = max(img.shape[1] for img in merged_df['image_data'])\n",
    "max_height = max(img.shape[0] for img in merged_df['image_data'])\n",
    "\n",
    "# Define a function to resize images while maintaining aspect ratio\n",
    "def resize_with_aspect_ratio(img, target_width, target_height):\n",
    "    # Calculate the scaling factor for width and height\n",
    "    width_ratio = target_width / img.shape[1]\n",
    "    height_ratio = target_height / img.shape[0]\n",
    "    # Choose the minimum scaling factor to maintain aspect ratio\n",
    "    min_ratio = min(width_ratio, height_ratio)\n",
    "    # Resize the image with the minimum scaling factor\n",
    "    resized_img = Image.fromarray(img).resize((int(img.shape[1] * min_ratio), int(img.shape[0] * min_ratio)))\n",
    "    return np.array(resized_img)\n",
    "\n",
    "# Apply resizing to all images in the DataFrame\n",
    "merged_df['resized_image_data'] = merged_df['image_data'].apply(lambda img: resize_with_aspect_ratio(img, max_width, max_height))\n",
    "merged_df.head()\n",
    "# Now, merged_df['resized_image_data'] contains resized images with maintained aspect ratio\n",
    "# You can convert these resized images back to a string representation if needed\n",
    "# merged_df['resized_image_data'] = merged_df['resized_image_data'].apply(lambda img: img.tolist())\n",
    "\n",
    "# Optionally, save the DataFrame to a new CSV file\n",
    "#merged_df.to_csv('resized_images.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(179,)"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df['image_data'].values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mDocstring:\u001b[0m\n",
      "Reshapes a tensor.\n",
      "\n",
      "Given `tensor`, this operation returns a new `tf.Tensor` that has the same\n",
      "values as `tensor` in the same order, except with a new shape given by\n",
      "`shape`.\n",
      "\n",
      ">>> t1 = [[1, 2, 3],\n",
      "...       [4, 5, 6]]\n",
      ">>> print(tf.shape(t1).numpy())\n",
      "[2 3]\n",
      ">>> t2 = tf.reshape(t1, [6])\n",
      ">>> t2\n",
      "<tf.Tensor: shape=(6,), dtype=int32,\n",
      "  numpy=array([1, 2, 3, 4, 5, 6], dtype=int32)>\n",
      ">>> tf.reshape(t2, [3, 2])\n",
      "<tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n",
      "  array([[1, 2],\n",
      "         [3, 4],\n",
      "         [5, 6]], dtype=int32)>\n",
      "\n",
      "The `tf.reshape` does not change the order of or the total number of elements\n",
      "in the tensor, and so it can reuse the underlying data buffer. This makes it\n",
      "a fast operation independent of how big of a tensor it is operating on.\n",
      "\n",
      ">>> tf.reshape([1, 2, 3], [2, 2])\n",
      "Traceback (most recent call last):\n",
      "...\n",
      "InvalidArgumentError: Input to reshape is a tensor with 3 values, but the\n",
      "requested shape has 4\n",
      "\n",
      "To instead reorder the data to rearrange the dimensions of a tensor, see\n",
      "`tf.transpose`.\n",
      "\n",
      ">>> t = [[1, 2, 3],\n",
      "...      [4, 5, 6]]\n",
      ">>> tf.reshape(t, [3, 2]).numpy()\n",
      "array([[1, 2],\n",
      "       [3, 4],\n",
      "       [5, 6]], dtype=int32)\n",
      ">>> tf.transpose(t, perm=[1, 0]).numpy()\n",
      "array([[1, 4],\n",
      "       [2, 5],\n",
      "       [3, 6]], dtype=int32)\n",
      "\n",
      "If one component of `shape` is the special value -1, the size of that\n",
      "dimension is computed so that the total size remains constant.  In particular,\n",
      "a `shape` of `[-1]` flattens into 1-D.  At most one component of `shape` can\n",
      "be -1.\n",
      "\n",
      ">>> t = [[1, 2, 3],\n",
      "...      [4, 5, 6]]\n",
      ">>> tf.reshape(t, [-1])\n",
      "<tf.Tensor: shape=(6,), dtype=int32,\n",
      "  numpy=array([1, 2, 3, 4, 5, 6], dtype=int32)>\n",
      ">>> tf.reshape(t, [3, -1])\n",
      "<tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n",
      "  array([[1, 2],\n",
      "         [3, 4],\n",
      "         [5, 6]], dtype=int32)>\n",
      ">>> tf.reshape(t, [-1, 2])\n",
      "<tf.Tensor: shape=(3, 2), dtype=int32, numpy=\n",
      "  array([[1, 2],\n",
      "         [3, 4],\n",
      "         [5, 6]], dtype=int32)>\n",
      "\n",
      "`tf.reshape(t, [])` reshapes a tensor `t` with one element to a scalar.\n",
      "\n",
      ">>> tf.reshape([7], []).numpy()\n",
      "7\n",
      "\n",
      "More examples:\n",
      "\n",
      ">>> t = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
      ">>> print(tf.shape(t).numpy())\n",
      "[9]\n",
      ">>> tf.reshape(t, [3, 3])\n",
      "<tf.Tensor: shape=(3, 3), dtype=int32, numpy=\n",
      "  array([[1, 2, 3],\n",
      "         [4, 5, 6],\n",
      "         [7, 8, 9]], dtype=int32)>\n",
      "\n",
      ">>> t = [[[1, 1], [2, 2]],\n",
      "...      [[3, 3], [4, 4]]]\n",
      ">>> print(tf.shape(t).numpy())\n",
      "[2 2 2]\n",
      ">>> tf.reshape(t, [2, 4])\n",
      "<tf.Tensor: shape=(2, 4), dtype=int32, numpy=\n",
      "  array([[1, 1, 2, 2],\n",
      "         [3, 3, 4, 4]], dtype=int32)>\n",
      "\n",
      ">>> t = [[[1, 1, 1],\n",
      "...       [2, 2, 2]],\n",
      "...      [[3, 3, 3],\n",
      "...       [4, 4, 4]],\n",
      "...      [[5, 5, 5],\n",
      "...       [6, 6, 6]]]\n",
      ">>> print(tf.shape(t).numpy())\n",
      "[3 2 3]\n",
      ">>> # Pass '[-1]' to flatten 't'.\n",
      ">>> tf.reshape(t, [-1])\n",
      "<tf.Tensor: shape=(18,), dtype=int32,\n",
      "  numpy=array([1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6],\n",
      "  dtype=int32)>\n",
      ">>> # -- Using -1 to infer the shape --\n",
      ">>> # Here -1 is inferred to be 9:\n",
      ">>> tf.reshape(t, [2, -1])\n",
      "<tf.Tensor: shape=(2, 9), dtype=int32, numpy=\n",
      "  array([[1, 1, 1, 2, 2, 2, 3, 3, 3],\n",
      "         [4, 4, 4, 5, 5, 5, 6, 6, 6]], dtype=int32)>\n",
      ">>> # -1 is inferred to be 2:\n",
      ">>> tf.reshape(t, [-1, 9])\n",
      "<tf.Tensor: shape=(2, 9), dtype=int32, numpy=\n",
      "  array([[1, 1, 1, 2, 2, 2, 3, 3, 3],\n",
      "         [4, 4, 4, 5, 5, 5, 6, 6, 6]], dtype=int32)>\n",
      ">>> # -1 is inferred to be 3:\n",
      ">>> tf.reshape(t, [ 2, -1, 3])\n",
      "<tf.Tensor: shape=(2, 3, 3), dtype=int32, numpy=\n",
      "  array([[[1, 1, 1],\n",
      "          [2, 2, 2],\n",
      "          [3, 3, 3]],\n",
      "         [[4, 4, 4],\n",
      "          [5, 5, 5],\n",
      "          [6, 6, 6]]], dtype=int32)>\n",
      "\n",
      "Args:\n",
      "  tensor: A `Tensor`.\n",
      "  shape: A `Tensor`. Must be one of the following types: `int32`, `int64`.\n",
      "    Defines the shape of the output tensor.\n",
      "  name: Optional string. A name for the operation.\n",
      "\n",
      "Returns:\n",
      "  A `Tensor`. Has the same type as `tensor`.\n",
      "\u001b[0;31mFile:\u001b[0m      ~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/ops/array_ops.py\n",
      "\u001b[0;31mType:\u001b[0m      function\n"
     ]
    }
   ],
   "source": [
    "tf.reshape?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Failed to convert a NumPy array to a Tensor (Unsupported object type tensorflow.python.framework.ops.EagerTensor).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[206], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m reshaped_images \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/framework/constant_op.py:98\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[0;34m(value, ctx, dtype)\u001b[0m\n\u001b[1;32m     96\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtypes\u001b[38;5;241m.\u001b[39mas_dtype(dtype)\u001b[38;5;241m.\u001b[39mas_datatype_enum\n\u001b[1;32m     97\u001b[0m ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 98\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mEagerTensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mValueError\u001b[0m: Failed to convert a NumPy array to a Tensor (Unsupported object type tensorflow.python.framework.ops.EagerTensor)."
     ]
    }
   ],
   "source": [
    "reshaped_images = tf.reshape(image_tensors, [256, 256])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2795, 2795, 2795, 2795, 2828, 2795, 2828, 2798, 2795, 2795, 2795, 2828, 2795, 2798, 2798, 2795, 2795, 2795, 2828, 2795, 2795, 2795, 2795, 2795, 2795, 2795, 2828, 2795, 2798, 2795, 2795, 2795, 2795, 2795, 2795, 2828, 2795, 2828, 2828, 2795, 2795, 2795, 2795, 2795, 2795, 2828, 2795, 2795, 2828, 2828, 2828, 2828, 2828, 2795, 2795, 2795, 2798, 2828, 2828, 2828, 2828, 2828, 2828, 2795, 2828, 2795, 2795, 2795, 2795, 2795, 2798, 2795, 2795, 2828, 2795, 2795, 2800, 2795, 2828, 2795, 2828, 2795, 2828, 2828, 2795, 2828, 2795, 2795, 2795, 2795, 2795, 2795, 2800, 2828, 2828, 2828, 2795, 2795, 2795, 2795, 2795, 2795, 2795, 2800, 2800, 2828, 2798, 2795, 2795, 2795, 2828, 2795, 2828, 2828, 2795, 2828, 2828, 2828, 2798, 2828, 2828, 2800, 2828, 2795, 2798, 2795, 2828, 2795, 2828, 2795, 2795, 2795, 2828, 2795, 2828, 2828, 2795, 2828, 2828, 2828, 2795, 2795, 2828, 2795, 2795, 2828, 2795, 2828, 2795, 2795, 2795, 2795, 2795, 2795, 2795, 2795, 2795, 2795, 2795, 2828, 2795, 2828, 2828, 2828, 2795, 2800, 2795, 2795, 2795, 2795, 2795, 2795, 2828, 2828, 2795, 2795, 2795, 2795, 2828]\n"
     ]
    }
   ],
   "source": [
    "resized_image_lengths = [len(image_data) for image_data in merged_df['resized_image_data'].values]\n",
    "print(resized_image_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the DataFrame into features (X) and labels (y)\n",
    "X = np.stack(X_bar)\n",
    "y = merged_df.drop(['filename', 'image_data','padded_image_data'], axis=1)\n",
    "print(X, y)\n",
    "\n",
    "# One-hot encode the labels\n",
    "y = pd.get_dummies(y, dummy_na=True)\n",
    "\n",
    "# Splitting the data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CNN architecture\n",
    "model = Sequential([\n",
    "    Conv2D(64, (3, 3), activation='relu', input_shape=X_train.shape[1:]),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D((2, 2)),\n",
    "    Flatten(),\n",
    "    Dropout(0.2),\n",
    "    Dense(512, activation='relu'),\n",
    "    Dense(3, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
